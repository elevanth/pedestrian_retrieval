# train note

recommend use typore to view this note

| date | checkpoint                  | model                                    | train                                    | loss                                     | result                                   | conclusion                               |
| ---- | --------------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- |
| 6.20 | model                       | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to 224*224, change VGG_MEAN, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.9 | base triplet, distance_alfa = 0.2        | 110000 step, best mAP 0.29               | base triplet is ok                       |
| 6.22 | model_0.4                   | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  **fine tuning all**, resize image to 224*224, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.96 | base triplet, distance_alfa = 0.2        | 65000 step, best mAP 0.2                 | fine turning all layers is slow but no much help |
| 6.23 | model_224_112               | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to **224*112**, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.96 | base triplet, distance_alfa = 0.2        | 230000 step, best mAP 0.3                | decrease cnn input size can not improve  |
| 6.24 | model_244_112               | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to 224*112, **split loss in a batch**, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.96 | base triplet, distance_alfa = 0.2        | 350000 step, best mAP 0.3                | SGD is not better than min-batch         |
| 6.25 | model_trip_improve          | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.9 | improved triplet, **tau1 = 0.4, tau2 = 0.01**, beta = 0.002 | 225000 step, best mAP 0.24 (when tau1 = 1.0, 30000 step, best mAP 0.1, mAP unstable) | tau1 = 0.2 better than 0.4 better than 1.0 |
| 6.26 | model_trip_improve          | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to 224*112, normalize output | AdamOptimizer, **initial_learning_rate = 0.00001**, learning_rate_decay_factor = 0.9 | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 35000 step, **mAP 0.4**, seams can improve with more step(train with normalize feature, compute mAP and predict without normalize) | 1. initial_learning_rate = 0.00001 is better than 0.01, when 0.01, features will be unconvergence and reach 10^10. 2. loss and net output can be used to select learning rate. |
| 6.26 | model_trip_improve          | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to 224*112, **no normalize** | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9 | improved triplet, tau1 = 0.2,0.4,1.0, tau2 = 0.01, beta = 0.002,0.02.0.2 | 1. when initial_learning_rate = 0.00001, 5000 step, mAP 0.24, 10000 step, mAP 0.28. 2.when initial_learning_rate = 0.0001, 5000 step, mAP 0.05, 10000 step, mAP 0.07. | without normalize, all features will tend to 0 to keep loss |
| 6.26 | model_trip_improve_1        | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9 | improved triplet, **tau1 = 1.0**, tau2 = 0.01, beta = 0.002 | 40000 step, mAP 0.14                     | tau1 = 0.4 better than 1.0               |
| 6.26 | model_trip_improve_0.2      | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9 | improved triplet, **tau1 = 0.2**, tau2 = 0.01, beta = 0.002 | 35000 step, best mAP 0.3                 | tau1 = 0.4 better than 0.2               |
| 6.26 | model_trip_improve_beta0.01 | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9 | improved triplet, tau1 = 0.4, tau2 = 0.01, **beta = 0.01** | 180000 step, valid best mAP 0.31, train set has mAP 0.35 | beta = 0.002 better then 0.01            |
| 6.27 | model_trip_improve_batch    | vgg19, delete conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning conv4,fc6,fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, **train_batch_size = 30** | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 5000 step, best mAP 0.31                 | big batch can speed up convergence       |
| 6.27 | model_trip_improve_conv5    | vgg19, delete fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning **conv5**,fc6,fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 30 | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 15000 step, mAP 0.32                     | add conv5 has a little help              |
| 6.27 | model_trip_improve_conv3    | vgg19, delete **conv4**,conv5,fc6,fc7,fc8, add fc6_new, fc7_new(100),  fine tuning **conv5**,fc6,fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 30 | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 5000 step, mAP 0.27                      | delete conv3 has a little demage         |





