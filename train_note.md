# train note

recommend use typore to view this note

| date    | checkpoint                  | model                                    | train                                    | loss                                     | result                                   | conclusion                               |
| ------- | --------------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- |
| 6.20    | model                       | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to 224*224, change VGG_MEAN, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.9, train_batch_size = 10 | base triplet, distance_alfa = 0.2        | 110000 step, best mAP 0.29               | base triplet is ok                       |
| 6.22    | model_0.4                   | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  **fine tuning all**, resize image to 224*224, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.96, train_batch_size = 10 | base triplet, distance_alfa = 0.2        | 65000 step, best mAP 0.2                 | fine turning all layers is slow but no much help |
| 6.23    | model_224_112               | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to **224*112**, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.96, train_batch_size = 10 | base triplet, distance_alfa = 0.2        | 230000 step, best mAP 0.3                | decrease cnn input size can not improve  |
| 6.24    | model_244_112               | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to 224*112, **split loss in a batch**, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.96, train_batch_size = 10 | base triplet, distance_alfa = 0.2        | 350000 step, best mAP 0.3                | SGD is not better than min-batch         |
| 6.25    | model_trip_improve          | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.001, learning_rate_decay_factor = 0.9, train_batch_size = 10 | improved triplet, **tau1 = 0.4, tau2 = 0.01**, beta = 0.002 | 225000 step, best mAP 0.24 (when tau1 = 1.0, 30000 step, best mAP 0.1, mAP unstable) | tau1 = 0.2 better than 0.4 better than 1.0 |
| 6.26    | model_trip_improve          | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, **initial_learning_rate = 0.00001**, learning_rate_decay_factor = 0.9, train_batch_size = 10 | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 35000 step, **mAP 0.4**, seams can improve with more step(train with normalize feature, compute mAP and predict without normalize) | 1. initial_learning_rate = 0.00001 is better than 0.01, when 0.01, features will be unconvergence and reach 10^10. 2. loss and net output can be used to select learning rate. |
| 6.26    | model_trip_improve          | vgg19, delete conv5, fc6, fc7,$ fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to 224*112, **no normalize** | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 10 | improved triplet, tau1 = 0.2,0.4,1.0, tau2 = 0.01, beta = 0.002,0.02.0.2 | 1. when initial_learning_rate = 0.00001, 5000 step, mAP 0.24, 10000 step, mAP 0.28. 2.when initial_learning_rate = 0.0001, 5000 step, mAP 0.05, 10000 step, mAP 0.07. | without normalize, all features will tend to 0 to keep loss |
| 6.26    | model_trip_improve_1        | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 10 | improved triplet, **tau1 = 1.0**, tau2 = 0.01, beta = 0.002 | 40000 step, mAP 0.14                     | tau1 = 0.4 better than 1.0               |
| 6.26    | model_trip_improve_0.2      | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 10 | improved triplet, **tau1 = 0.2**, tau2 = 0.01, beta = 0.002 | 35000 step, best mAP 0.3                 | tau1 = 0.4 better than 0.2               |
| 6.26    | model_trip_improve_beta0.01 | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 10 | improved triplet, tau1 = 0.4, tau2 = 0.01, **beta = 0.01** | 180000 step, valid best mAP 0.31, train set has mAP 0.35 | beta = 0.002 better then 0.01            |
| 6.27    | model_trip_improve_batch    | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning conv4, fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, **train_batch_size = 30** | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 5000 step, best mAP 0.31                 | big batch can speed up convergence       |
| 6.27    | model_trip_improve_conv5    | vgg19, delete fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning **conv5**, fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 30 | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 15000 step, mAP 0.32                     | add conv5 has a little help              |
| 6.27    | model_trip_improve_evgloss  | vgg19, delete **conv3**, conv4, conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 30 | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 5000 step, mAP 0.27                      | delete conv3 has a little demage         |
| 6.27    | model_trip_improve_evgloss  | vgg19, delete conv4, conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 30 | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002, **delete moving average** | 10000 step, mAP 0.17                     | use moving average is better             |
| 6.27    | model_trip_improve_small    | vgg19, delete conv4, conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning fc6, fc7, resize image to **112*48**, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 30 | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 10000 step, mAP 0.18                     | small net input is worse                 |
| 6.27    | model_trip_improve_small    | vgg19, delete conv4, conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning fc6, fc7, resize image to **112*56**, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = **90** | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 10000 step, mAP 0.177                    | big batch useless, but can speed up convergence |
| 6.27    | model_trip_improve_big      | vgg19, delete conv4, conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning fc6, fc7, resize image to **320*128**, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 30 | improved triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | 15000 step, mAP 0.20                     | big net input is worse                   |
| 6.28    | model_trip_improve_re       | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new, fc7_new(100),  fine tuning fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, **train_batch_size = 4** | improved triplet, tau1 = 0.4, **tau2 = 0.01**, beta = 0.002, delete loss moving average | 65000 step, mAP 0.18                     |                                          |
| 6.28    | model_trip_improve_relu     | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new(**1024**), fc7_new(100, **add relu**),  fine tuning fc6, fc7, resize image to 224*112, normalize output | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 10 | improved triplet, tau1 = 0.4, tau2 = 0.001, beta = 0.002 | 30000 step, mAP0.05                      | add relu is worse                        |
| 6.29    | model_trip_improve_nodrop   | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new(256), fc7_new(100),  fine tuning fc6, fc7, resize image to 224*112, random input, normalize output, **drop out=0.9,1.0** | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, **train_batch_size = 8** | improved triplet, tau1 = 0.4, tau2 = 0.001, beta = 0.002 | 15000 step, mAP 0.29, drop=0.9, (drop=1.0, NaN at 1100 step) | big dropout is better, but 1.0 will be error |
| 6.29    | model_trip_improve_512fc    | vgg19, delete conv5, fc6, fc7, fc8, add **fc6_new(1024), fc7_new(512)**,  fine tuning fc6, fc7, resize image to 224*112, random input, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 8 | improved triplet, tau1 = 0.4, tau2 = 0.001, beta = 0.002 | 15000 step, mAP 0.32                     | increse dim is better                    |
| 6.29    | model_trip_improve_max      | vgg19, delete conv5, fc6, fc7, fc8, add **fc6_new(1024), fc7_new(512)**,  fine tuning fc6, fc7, resize image to 224*112, random input, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 8 | **improved max triplet**, tau1 = 0.4, tau2 = 0.001, beta = 0.002 | **75000 step, mAP 0.41**                 | improved max triplet is better           |
| 6.29    | trip_improve_max1024        | vgg19, delete conv5, fc6, fc7, fc8, add **fc6_new(2048), fc7_new(1024)**,  fine tuning fc6, fc7, resize image to 224*112, random input, normalize output, **drop out=0.8** | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 8 | improved max triplet, tau1 = 0.4, tau2 = 0.001, beta = 0.002 | **100000 step, mAP 0.42**                | increse dim is better, but improve is not much |
| 6.30    | trip_improve_crop           | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new(1024), fc7_new(512),  fine tuning fc6, fc7, **resize and crop image to 224*112**, random input, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 8 | improved max triplet, tau1 = 0.4, tau2 = 0.001, beta = 0.002 | **20000 step, mAP 0.35**                 | crop seams good                          |
| 6.30    | trip_improve_crop_1         | vgg19, delete conv5, fc6, fc7, fc8, add fc6_new(1024), fc7_new(512),  fine tuning fc6, fc7, resize and crop image to 224*112, random input, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.00001, learning_rate_decay_factor = 0.9, train_batch_size = 8 | improved max triplet, **tau1 = 1.0**, tau2 = 0.001, beta = 0.002 | 40000 step, mAP 0.17                     | tau1 = 1.0 is worse                      |
| 6.30    | trip_improve_mine           | **build conv1, pool1, conv2, pool2, fc3(512)**, resize and crop image to 224*112, random input, normalize output, drop out=0.9 | AdamOptimizer, **initial_learning_rate = 0.0001**, learning_rate_decay_factor = 0.9, **train_batch_size = 16** | improved max triplet, **tau1 = 0.5**, **tau2 = 0.01**, beta = 0.002 | **5000 step, mAP 0.41**                  | use small net is good                    |
| 6.30    | trip_improve_mine2          | build conv1, pool1, conv2, pool2, **fc3(1024), fc4(512)**, resize and crop image to 224*112, random input, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, **train_batch_size = 8** | improved max triplet, tau1 = 0.5, tau2 = 0.01, beta = 0.002 | 10000 step, mAP 0.39                     | deep net may train slow                  |
| 6.30    | trip_improve_mine1024       | **build conv1, pool1, conv2, pool2, fc3(1024)**, resize and crop image to 224*112, random input, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, **train_batch_size = 16** | improved max triplet, tau1 = 0.5, tau2 = 0.01, beta = 0.002 | **5000 step, mAP 0.44**                  | increse dim is better                    |
| 6.30    | trip_improve_mine1024       | build conv1, pool1, conv2, pool2, fc3(1024), resize and crop image to 224*112, random input, normalize output, drop out=0.9 | AdamOptimizer, **initial_learning_rate = 0.001**, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, **tau1 = 0.7**, tau2 = 0.01, beta = 0.002 | step 10000, mAP 0.39                     | tau1 = 0.8 is worse                      |
| 6.30    | trip_improve_mine_norelu    | build conv1, pool1, conv2, pool2, fc3(1024,**no relu**), resize and crop image to 224*112, random input, normalize output, drop out=0.9 | AdamOptimizer, **initial_learning_rate = 0.0001**, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, **tau1 = 0.4**, tau2 = 0.01, beta = 0.002 | **step 15000, mAP 0.55**                 | no relu is better                        |
| 6.30    | trip_improve_mine_norelu    | build conv1, pool1, conv2, pool2, fc3(1024, no relu), resize and crop image to 224*112, random input, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, tau1 = 0.4, tau2 = 0.01, **beta = 0.1** | step 15000, mAP 0.5                      | beta should not be to big                |
| 6.30    | trip_improve_mine_pre       | build conv1, pool1, conv2, pool2, fc3(1024, no relu), resize and crop image to 224*112, random input, **add preprecess**, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 15000, mAP 0.52                     | image preprecess seems no use            |
| 6.30    | trip_improve_mine_mid       | build conv1, pool1, conv2, pool2, fc3(1024, no relu), resize and crop image to 224*112, random input, add preprecess, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | **improved max medial triplet**, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 10000, mAP 0.47                     | improved max medial triplet seems worse  |
| 6.30    | trip_improve_mine_avg       | build conv1, pool1, conv2, pool2, fc3(1024, no relu), resize and crop image to 224*112, random input, add preprecess, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | **improved avg triplet**, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 10000, mAP 0.49                     | improved avg triplet seems not the best  |
| 6.30    | trip_improve_mine_fc4       | build conv1, pool1, conv2, pool2, fc3(1024), **fc4(1024)**, resize and crop image to 224*112, random input, add preprecess, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = **128** | improved max triplet, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 5000, mAP 0.51                      | will overfitting                         |
| 7.1     | trip_improve_minefc4        | **vgg19, fining turn fc(1000)**, resize and crop image to **224*224**, random input, add preprecess, normalize output, **drop out=0.8** | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = **8** | improved max triplet, no evg_loss, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 20000, mAP 0.19                     | vgg is worse                             |
| 7.1     | trip_improve_mine_noevg     | build conv1, pool1, conv2, pool2, fc3(1024, no relu), resize and crop image to 224*112, random input, **add preprecess**, normalize output, drop out=0.9 | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, **no evg_loss**, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | **step 25000, mAP 0.56 (top4 0.84)**     | no evg_loss is better, pick top 4 is better |
| 7.2     | trip_improve_mine_float     | build conv1, pool1, conv2, pool2, fc3(1024, no relu), resize and crop image to 224*112, random input, add preprecess, normalize output, drop out=0.9, **use float** | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, no evg_loss, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | **step 15000, mAP 0.56**                 | float is better                          |
| 7.2     | trip_improve_mine_nobias    | build conv1, pool1, conv2, pool2, fc3(1024, no relu), **all layers no bias**, resize and crop image to 224*112, random input, add preprecess, normalize output, drop out=0.9, use float | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, no evg_loss, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 15000, mAP 0.45                     | use bias is better                       |
| 7.2     | trip_improve_mine_init      | build conv1, pool1, conv2, pool2, fc3(1024, no relu), resize and crop image to 224*112, random input, add preprecess, normalize output, drop out=0.9, use float, **normal initialize** | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, no evg_loss, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 15000, mAP 0.54                     | normal initialize seams no use           |
| 7.2     | trip_improve_mine_drop      | build conv1, pool1, conv2, pool2 **(drop out)**, fc3(1024, no relu), resize and crop image to 224*112, random input, add preprecess, normalize output, **drop out=0.9**, use float | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, no evg_loss, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 15000, mAP 0.41                     | move drop out seams not good             |
| 7.2     | trip_improve_mine_droprelu  | build conv1, pool1, conv2, pool2**(drop out)** , fc3(1024, add relu), resize and crop image to 224*112, random input, add preprecess, normalize output, drop out=0.9, use float | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, no evg_loss, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 15000, mAP 0.52                     | move drop, add relu is ok                |
| 7.2     | trip_improve_mine_64        | build conv1, pool1, conv2 **(64)**, pool2 , fc3 (1024, no relu), resize and crop image to 224*112, random input, add preprecess, normalize output, drop out=0.9, use float | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = **32** | improved max triplet, no evg_loss, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 5000, best mAP 0.53                 | speed up convergence, will over fitting  |
| 7.2     | trip_improve_mine_95        | build conv1, pool1, conv2, pool2, fc3(1024, no relu), resize and crop image to 224*112, random input, add preprecess, normalize output, **drop out=0.95**, use float | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet, no evg_loss, tau1 = 0.4, tau2 = 0.01, beta = 0.002 | step 15000, mAP 0.55                     | increase dropout seams no use            |
| 7.2     | trip_improve_mine_squ       | build conv1, pool1, conv2, pool2, fc3(1024, no relu), resize and crop image to 224*112, random input, add preprecess, normalize output, drop out=0.9, use float | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet **(square dist)**, no evg_loss, **tau1 = 1.0, 0.8, 0.6**, tau2 = 0.01, beta = 0.002 | step 15000, mAP 0.55(tau1 = 1.0), step 15000, **mAP 0.57(tau2 = 0.8)**, **step 10000, mAP 0.57(tau2 = 0.6)** | square dist seams ok                     |
| 7.3**** | **trip_improve_mul**        | build as literature,concat fc (**800**, no relu), n_fc (**800**, no relu), resize and crop image to **230*80**, random input, add preprecess, normalize output, **no dropout**, use float | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet (square dist), no evg_loss, **tau1 = 0.6, 0.8, 0.5**, tau2 = 0.01, beta = 0.002 | step 15000, mAP 0.56 (with n_fc), **step 15000, mAP 0.58(no n_fc, tau1 = 0.6)**, step 20000, mAP 0.51(no n_fc, tau1 = 0.8), step 20000, mAP 0.56(no n_fc, tau1 = 0.5) | mulit-channel is good                    |
| 7.3     | trip_improve_mul            | build as literature,concat fc (**1000**, no relu), resize and crop image to 230*80, random input, add preprecess, normalize output, no dropout, use float | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, train_batch_size = 16 | improved max triplet (square dist), no evg_loss, tau1 = 0.6, tau2 = 0.01, beta = 0.002 | step 25000, mAP 0.53                     | increase dim is worse                    |
| 7.3     | trip_improve_joint          | build as literature, resize and crop image to **160*60**, random input, add preprecess, no dropout, use float | AdamOptimizer, initial_learning_rate = 0.0001, learning_rate_decay_factor = 0.9, **train_batch_size = 64** | SIR+CIR loss, no evg_loss, tau1 = 0.6, tau2 = 0.01, beta = 0.002, epsilon = 0.6, eta = 1.0, lambda_ = 1.0 | **step 30000, mAP 0.66**                 | epsilon 1.0 better than 0.6              |